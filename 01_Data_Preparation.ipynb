{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの準備\r\n",
    "---\r\n",
    "\r\n",
    "このリポジトリでは、[Azure Open Datasets](https://azure.microsoft.com/services/open-datasets/) のシミュレートされたオレンジ ジュースの売上データを使用して、Azure Machine Learning で Many models をトレーニングし、予測するプロセスを説明します。\r\n",
    "\r\n",
    "このノートブックでは、このソリューション アクセラレータのデータを構成するために必要なすべての手順について説明します:\r\n",
    "\r\n",
    "1. サンプル データのダウンロード\r\n",
    "2. トレーニング/予測セットの分割\r\n",
    "3. ワークスペースに接続し、データをデータストアにアップロード\r\n",
    "\r\n",
    "### 前提条件\r\n",
    "[00_Setup_AML_Workspace](00_Setup_AML_Workspace.ipynb) ノートブックを既に実行している場合は、すべて設定済みです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 サンプル データのダウンロード\r\n",
    "\r\n",
    "この例で使用される時系列データは、シカゴ大学によるドミニク・ファイン・フーズ・データセットに基づいてシミュレートされ、個々の店舗で3つの異なるオレンジジュースブランドの2年間の売上に着目しています。データセットの詳細については、[こちら](https://azure.microsoft.com/services/open-datasets/catalog/sample-oj-sales-simulated/)をご覧ください。\r\n",
    "\r\n",
    "完全なデータセットには、3つのオレンジジュースブランドを持つ3,991店舗のシミュレートされた売上が含まれており、11,973のモデルが多くのモデルパターンの力を発揮できるようにトレーニングすることができます。各時系列データには、'1990-06-14'から'1992-10-01'までのデータが含まれています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データをダウンロードするには、`azureml-opendatasets` パッケージが必要です。次の方法でインストールできます:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install azureml-opendatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install update azureml-opendatasets==1.10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最初の 10 個のファイルをダウンロードするところから始めますが、以下のコードを編集して、簡単に11,973 モデルすべてをトレーニングできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_maxfiles = 10 # 11973 または 0 に設定すると、すべてのファイルを取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\r\n",
    "from azureml.opendatasets import OjSalesSimulated\r\n",
    "\r\n",
    "# 全てのデータをプル\r\n",
    "oj_sales_files = OjSalesSimulated.get_file_dataset()\r\n",
    "\r\n",
    "# `dataset_maxfiles` で指定した最初のファイル数のみプル\r\n",
    "if dataset_maxfiles:\r\n",
    "    oj_sales_files = oj_sales_files.take(dataset_maxfiles)\r\n",
    "\r\n",
    "# ダウンロード先のフォルダを作成\r\n",
    "target_path = 'oj_sales_data' \r\n",
    "os.makedirs(target_path, exist_ok=True)\r\n",
    "\r\n",
    "# データのダウンロード\r\n",
    "oj_sales_files.download(target_path, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 データを2 つのセットに分割\r\n",
    "\r\n",
    "次に、各データセットを 2 つに分割します。トレーニングファイルには'1992-5-28'以前のデータレコードが含まれ、各時系列データの残りの部分は推論ファイルに保存されます。\r\n",
    "\r\n",
    "最後に、両方のデータ ファイルのセットをワークスペースの既定の[データストア](https://docs.microsoft.compython/api/azureml-core/azureml.core.datastore(class))にアップロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.helper import split_data\r\n",
    "\r\n",
    "# 推論データセットに分割する日付と、データ内のタイムスタンプ列の名前を指定\r\n",
    "timestamp_column = 'WeekStarting'\r\n",
    "split_date = '1992-05-28'\r\n",
    "\r\n",
    "# 各ファイルを分割し、対応するディレクトリに格納\r\n",
    "train_path, inference_path = split_data(target_path, timestamp_column, split_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 AML ワークスペースのデータストアへのデータのアップロード\r\n",
    "\r\n",
    "[ノートブックのセットアップ](00_Setup_AML_Workspace.ipynb)で[ワークスペース](https://docs.microsoft.com/ja-jp/python/api/azureml-core/azureml.core.workspace.workspace?view=azure-ml-py)を作成しました。ここでは、その環境にデータを登録します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace\r\n",
    "\r\n",
    "ws = Workspace.from_config()\r\n",
    "\r\n",
    "# ワークスペースを確認\r\n",
    "ws.get_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "両方のデータ ファイルのセットがワークスペースのデフォルトの[データストア](https://docs.microsoft.com/ja-jp/azure/machine-learning/how-to-access-data)にアップロードされます。データストアは、トレーニングや予測のためにデータを保存、アクセスできる場所です。データストアからデータにアクセスする方法については、[データストアのドキュメント](https://docs.microsoft.com/ja-jp/python/api/azureml-core/azureml.core.datastore(class)?view=azure-ml-py)を参照してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# デフォルトのデータストアに接続\r\n",
    "datastore = ws.get_default_datastore()\r\n",
    "\r\n",
    "# トレーニングデータのアップロード\r\n",
    "ds_train_path = target_path + '_train'\r\n",
    "datastore.upload(src_dir=train_path, target_path=ds_train_path, overwrite=True)\r\n",
    "\r\n",
    "# 推論データのアップロード\r\n",
    "ds_inference_path = target_path + '_inference'\r\n",
    "datastore.upload(src_dir=inference_path, target_path=ds_inference_path, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *[オプション]* データが既に Azure にある場合: そこからデータストアを作成する\r\n",
    "\r\n",
    "データが既に Azure に存在する場合は、ローカル コンピューターから既定のデータストアにアップロードする必要はありません。代わりに、そのデータのセットを参照する新しいデータストアを作成できます。次に、サンプル データが格納されている BLOB ストレージ内のコンテナーからデータストアを設定する方法の例を示します。\r\n",
    "\r\n",
    "この場合、オレンジジュースデータは、以下の情報で定義されたパブリック BLOB コンテナーで使用できます。この場合は、アカウントの資格情報も指定する必要があります。詳細については、[このドキュメント](https://docs.microsoft.com/ja-jp/python/api/azureml-core/azureml.core.datastore.datastore?view=azure-ml-py#register-azure-blob-container-workspace--datastore-name--container-name--account-name--sas-token-none--account-key-none--protocol-none--endpoint-none--overwrite-false--create-if-not-exists-false--skip-validation-false--blob-cache-timeout-none--grant-workspace-access-false--subscription-id-none--resource-group-none-)を参照してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "automl"
    ]
   },
   "outputs": [],
   "source": [
    "blob_datastore_name = \"automl_many_models\"\n",
    "container_name = \"automl-sample-notebook-data\"\n",
    "account_name = \"automlsamplenotebookdata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "automl"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core import Datastore\n",
    "\n",
    "datastore = Datastore.register_azure_blob_container(\n",
    "    workspace=ws, \n",
    "    datastore_name=blob_datastore_name, \n",
    "    container_name=container_name,\n",
    "    account_name=account_name,\n",
    "    create_if_not_exists=True\n",
    ")\n",
    "\n",
    "if 0 < dataset_maxfiles < 11973:\n",
    "    ds_train_path = 'oj_data_small/'\n",
    "    ds_inference_path = 'oj_inference_small/'\n",
    "else:\n",
    "    ds_train_path = 'oj_data/'\n",
    "    ds_inference_path = 'oj_inference/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 AML ワークスペースにデータセットを登録する\r\n",
    "\r\n",
    "最後の手順は、トレーニング セットと推論セット用の[データセット](https://docs.microsoft.com/ja-jp/azure/machine-learning/concept-data#datasets)を Azure Machine Learning で作成および登録することです。\r\n",
    "\r\n",
    "現在、[FileDataset](https://docs.microsoft.com/ja-jp/python/api/azureml-core/azureml.data.file_dataset.filedataset?view=azure-ml-py) を使用することが多くのモデル パターンを利用する最良の方法であるため、次のセルで FileDataset を作成します。その後、ワークスペースに FileDataset を[登録](https://docs.microsoft.com/ja-jp/azure/machine-learning/how-to-create-register-datasets#register-datasets)します。これにより、トレーニング/推論セットは、後でモデルをトレーニングして予測を生成するときに簡単に参照できる単純な名前に関連付けられます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\r\n",
    "\r\n",
    "# FileDatasetsの作成\r\n",
    "ds_train = Dataset.File.from_files(path=datastore.path(ds_train_path), validate=False)\r\n",
    "ds_inference = Dataset.File.from_files(path=datastore.path(ds_inference_path), validate=False)\r\n",
    "\r\n",
    "# FileDatasetsの登録\r\n",
    "dataset_name = 'oj_data_small' if 0 < dataset_maxfiles < 11973 else 'oj_data'\r\n",
    "train_dataset_name = dataset_name + '_train'\r\n",
    "inference_dataset_name = dataset_name + '_inference'\r\n",
    "ds_train.register(ws, train_dataset_name, create_new_version=True)\r\n",
    "ds_inference.register(ws, inference_dataset_name, create_new_version=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 *[オプション]* 登録されたデータセットの操作\r\n",
    "\r\n",
    "データを登録した後、下記のコマンドを使って簡単に呼び出すことができます。これは、データセットが今後のノートブックでアクセスされる方法です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oj_ds = Dataset.get_by_name(ws, name=train_dataset_name)\n",
    "oj_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "登録されたデータセットからデータをダウンロードすることもできます:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_paths = oj_ds.download()\n",
    "download_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データ ファイルの 1 つを読み込んで、フォーマットを確認してみましょう:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sample_data = pd.read_csv(download_paths[0])\n",
    "sample_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 次のステップ\r\n",
    "\r\n",
    "データセットを作成したので、トレーニング ノートブックの 1 つに移動してモデルをトレーニングし、スコアを付ける準備が整いました:\r\n",
    "\r\n",
    "- 自動化ML: [02_AutoML_Training_Pipeline.ipynb](Automated_ML/02_AutoML_Training_Pipeline/02_AutoML_Training_Pipeline.ipynb) を開いてください。\r\n",
    "- カスタム スクリプト: [02_CustomScript_Training_Pipeline.ipynb](Custom_Script/02_CustomScript_Training_Pipeline.ipynb) を開いてください。"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "roastala"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}