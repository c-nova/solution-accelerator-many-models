{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 予測 パイプライン - カスタム スクリプト\r\n",
    "---\r\n",
    "\r\n",
    "このノートブックでは、前回のステップでトレーニングしたモデルを使用して販売のバッチ予測を行うパイプラインを作成します。設定する予測パイプラインは、前回の手順で作成したトレーニング パイプラインに似ていますので、ドキュメントを簡易的に記載します。手順と機能の詳細については、以前のノートブックを参照してください。\r\n",
    "\r\n",
    "### 前提条件\r\n",
    "この時点で、次の内容が完了している必要があります:\r\n",
    "\r\n",
    "1. [00_Setup_AML_Workspace notebook](../../00_Setup_AML_Workspace.ipynb) を使用して AML ワークスペースが作成作成済みであること\r\n",
    "2. [01_Data_Preparation.ipynb](../../01_Data_Preparation.ipynb) を実行してデータセットが作成済みであること\r\n",
    "3. [02_CustomScript_Training_Pipeline.ipynb](02_CustomScript_Training_Pipeline.ipynb) を実行してモデルがトレーニング済みであること"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Azure ML SDK の最新バージョンを使用していることを確認し、パイプライン ステップ パッケージをインストールしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade azureml-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install azureml-pipeline-steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 ワークスペースとデータストアへの接続"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\r\n",
    "from azureml.core import Datastore\r\n",
    "\r\n",
    "ws = Workspace.from_config()\r\n",
    "\r\n",
    "# データストアのセットアップ\r\n",
    "dstore = ws.get_default_datastore()\r\n",
    "\r\n",
    "print('Workspace Name: ' + ws.name, \r\n",
    "      'Azure Region: ' + ws.location, \r\n",
    "      'Subscription Id: ' + ws.subscription_id, \r\n",
    "      'Resource Group: ' + ws.resource_group, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 実験の作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment = Experiment(ws, 'forecasting_pipeline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 データセットの取得\r\n",
    "\r\n",
    "[データ準備ノートブック](../01_Data_Preparation.ipynb)では、予測目的でオレンジジュースのサブセットを登録しました。ここではデータストアから、そのデータセットへの参照を取得します。[モデリング ノートブック](02_CustomScript_Training_Pipeline.ipynb)でトレーニングされたモデルを使用して、各推論ファイルのすべての行に対する予測を生成します。\r\n",
    "\r\n",
    "11,973行の時系列データのファイルのサブセット、または完全なデータセットでパイプラインを実行することを選択できます。ファイルのサブセットのみを使用するように選択した場合は、トレーニング データセット名を `oj_data_small_inference` と指定します。それ以外の場合は、`oj_data_inference` を指定します。小さいデータセットから始めることをお勧めします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'oj_data_small_inference'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "dataset = Dataset.get_by_name(ws, name=dataset_name)\n",
    "dataset_input = dataset.as_named_input(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 予測パイプライン用の ParallelRunStep の作成\r\n",
    "トレーニング パイプラインで行ったように、ParallelRunStep を作成して予測プロセスを並列化します。このコードは、前回の手順と基本的に同じですが、train.py ではなく [**forecast.py**](scripts/forecast.py) を並列化します。引き続き時系列スキーマ (タイムスタンプ列名、時系列 ID 列名など) を予測スクリプトに渡す必要があることに注意してください。\r\n",
    "\r\n",
    "ただしトレーニング スクリプトとは異なり、予測スクリプトにターゲット列の名前は必要ありません。実際の予測シナリオではターゲットの実際の値は存在しないため、予測パイプラインは予測値を返します。ただし、予測パイプラインは、推論データセットに存在する場合であっても、実績値を返すことができます。\r\n",
    "\r\n",
    "### 4.1 ParallelRunStep の環境を設定する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "forecast_env = Environment(name=\"many_models_environment\")\n",
    "forecast_conda_deps = CondaDependencies.create(pip_packages=['sklearn', 'pandas', 'joblib', 'azureml-defaults', 'azureml-core', 'azureml-dataprep[fuse]'])\n",
    "forecast_env.python.conda_dependencies = forecast_conda_deps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 コンピュート ターゲットの選択"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これは[セットアップ ノートブック](../00_Setup_AML_Workspace.ipynb#3.0-Create-compute-cluster)で作成したコンピュート クラスタです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_cluster_name = \"cpucluster\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute\n",
    "\n",
    "compute = AmlCompute(ws, cpu_cluster_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 ParallelRunConfig の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import ParallelRunConfig \n",
    "\n",
    "process_count_per_node = 6\n",
    "node_count = 1\n",
    "timeout = 180\n",
    "\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory='./scripts',\n",
    "    entry_script='forecast.py',\n",
    "    mini_batch_size='1',\n",
    "    run_invocation_timeout=timeout, \n",
    "    error_threshold=10,\n",
    "    output_action='append_row', \n",
    "    environment=forecast_env, \n",
    "    process_count_per_node=process_count_per_node, \n",
    "    compute_target=compute, \n",
    "    node_count=node_count\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 ParallelRunStep の構成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineData\n",
    "from azureml.pipeline.steps import ParallelRunStep \n",
    "\n",
    "output_dir = PipelineData(name='forecasting_output', datastore=dstore)\n",
    "\n",
    "parallel_run_step = ParallelRunStep(\n",
    "    name=\"many-models-forecasting\",\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    inputs=[dataset_input],\n",
    "    output=output_dir,\n",
    "    allow_reuse=False,\n",
    "    arguments=['--timestamp_column', 'WeekStarting',\n",
    "               '--timeseries_id_columns', 'Store', 'Brand',\n",
    "               '--model_type', 'lr']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 予測をコピーするステップの作成\r\n",
    "\r\n",
    "予測パイプラインには、*parallel_run_step.txt* から別のコンテナー内の CSV ファイルに予測をコピーする 2 番目の手順が含まれています。この手順は簡単ですが、パイプラインにステップを追加して、予測を別のデータストアにアップロードしたり、出力に追加の変換を行う方法を示します。\r\n",
    "\r\n",
    "### 5.1 データ参照の作成\r\n",
    "まず、パイプラインの出力を保持し、それに対する参照を取得する **predictions** という名前のデータストアを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.data.data_reference import DataReference\n",
    "\n",
    "output_dstore = Datastore.register_azure_blob_container(\n",
    "    workspace=ws, \n",
    "    datastore_name=\"predictions\",\n",
    "    container_name=\"predictions\",\n",
    "    account_name=dstore.account_name,\n",
    "    account_key=dstore.account_key,\n",
    "    create_if_not_exists=True\n",
    ")\n",
    "\n",
    "output_dref = DataReference(output_dstore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 PythonScriptStep の作成\r\n",
    "次に、[PythonScriptStep](https://docs.microsoft.com/ja-jp/python/api/azureml-pipeline-steps/azureml.pipeline.steps.python_script_step.pythonscriptstep?view=azure-ml-py) を定義し、新しく作成したデータストアと *parallel_run_step.txt* の場所を指定します。コピー スクリプトは時系列スキーマも使用します。その理由は、コピー スクリプトが予測データのヘッダー行を作成するため、列名を知る必要があるためです。ターゲット列は、推論に使用されたデータに存在していたため、ここに渡されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "upload_predictions_step = PythonScriptStep(\n",
    "    name=\"copy_predictions\",\n",
    "    script_name=\"copy_predictions.py\",\n",
    "    compute_target=compute,\n",
    "    source_directory='./scripts',\n",
    "    inputs=[output_dref, output_dir],\n",
    "    allow_reuse=False,\n",
    "    arguments=['--parallel_run_step_output', output_dir,\n",
    "               '--output_dir', output_dref,\n",
    "               '--target_column', 'Quantity',\n",
    "               '--timestamp_column', 'WeekStarting',\n",
    "               '--timeseries_id_columns', 'Store', 'Brand']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.0 パイプラインの実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=[parallel_run_step, upload_predictions_step])\n",
    "run = experiment.submit(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前の予測実行への参照を取得するには、次のコードをコメント解除します。\r\n",
    "#from azureml.pipeline.core import PipelineRun\r\n",
    "#run = PipelineRun(experiment, '<pipeline run id>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実行が完了するまで待機\r\n",
    "run.wait_for_completion(show_output=False, raise_on_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.0 予測パイプラインの結果を表示する\r\n",
    "予測を確認するために、*parallel_run_step.txt* をダウンロードし、結果をデータフレームに読み込み、予測を視覚化します。上記で作成した予測コンテナから結果をダウンロードすることもできます。\r\n",
    "\r\n",
    "### 7.1 ローカルに parallel_run_step.txt をダウンロードする\r\n",
    "Azure Machine Learning クラスターに送信された実行が完了するまで待つ必要があります。実行ステータスは、https://ml.azure.com でモニターできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def download_predictions(run, target_dir=None, step_name='many-models-forecasting', output_name='forecasting_output'):\n",
    "    stitch_run = run.find_step_run(step_name)[0]\n",
    "    port_data = stitch_run.get_output_data(output_name)\n",
    "    port_data.download(target_dir, show_progress=True, overwrite=True)\n",
    "    return os.path.join(target_dir, 'azureml', stitch_run.id, output_name)\n",
    "\n",
    "file_path = download_predictions(run, 'output')\n",
    "file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 ファイルを dataframe に変換する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(file_path + '/parallel_run_step.txt', sep=\" \", header=None)\n",
    "df.columns = ['WeekStarting', 'Predictions', 'Quantity', 'Store', 'Brand']\n",
    "df['WeekStarting'] = pd.to_datetime(df['WeekStarting'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 予測を視覚化する\r\n",
    "まず、ブランド別予測数量の分布を見てみましょう："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "fig = sns.violinplot(x=df['Brand'], y=df['Predictions'], data=df)\n",
    "fig.set_title('Predictions by Brand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、これらの予測を時間の経過とともに見てみましょう："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "week = df.groupby(['WeekStarting', 'Brand'])\n",
    "week = week['Predictions'].sum()\n",
    "week = pd.DataFrame(week.unstack(level=1))\n",
    "\n",
    "week.plot()\n",
    "plt.title('Total Predictions by Brand')\n",
    "plt.xticks(rotation=40)\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('Total Predictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "そこから、結果をトリミングして個々のブランドを見ることができます："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "store = 1001\n",
    "df_1001 = df[df['Store'] == store]\n",
    "\n",
    "brands = df_1001.groupby(['WeekStarting','Brand'])\n",
    "brands= brands['Predictions'].sum()\n",
    "brands= pd.DataFrame(brands.unstack(level=1))\n",
    "\n",
    "brands.plot()\n",
    "plt.legend(loc='upper right', labels=brands.columns.values)\n",
    "plt.xticks(rotation=40)\n",
    "plt.title('Predictions for Store 1001')\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('Predicted Quantity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.0 パイプラインの発行とスケジュール (オプション)\r\n",
    "\r\n",
    "\r\n",
    "### 8.1 パイプラインを発行する\r\n",
    "満足できるパイプラインを作成したら、パイプラインを発行して、後からプログラムで呼び出すことができます。パイプラインの発行と呼び出しの詳細については、この[チュートリアル](https://docs.microsoft.com/ja-jp/azure/machine-learning/how-to-create-machine-learning-pipelines#publish-a-pipeline)を参照してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# published_pipeline = pipeline.publish(name = 'forecast_many_models',\n",
    "#                                      description = 'forecast many models',\n",
    "#                                      version = '1',\n",
    "#                                      continue_on_step_failure = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 パイプラインのスケジュール実行\r\n",
    "また、時間ベースまたは変更ベースのスケジュールで実行するように[パイプラインをスケジュール](https://docs.microsoft.com/ja-jp/azure/machine-learning/how-to-trigger-published-pipeline)することもできます。これは、毎月、またはデータドリフトなどの別のトリガーに基づいて、モデルを自動的に再トレーニングするために使用できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from azureml.pipeline.core import Schedule, ScheduleRecurrence\n",
    "    \n",
    "# training_pipeline_id = published_pipeline.id\n",
    "\n",
    "# recurrence = ScheduleRecurrence(frequency=\"Week\", interval=1, start_time=\"2020-01-01T09:00:00\")\n",
    "# recurring_schedule = Schedule.create(ws, name=\"forecasting_pipeline_recurring_schedule\", \n",
    "#                             description=\"Schedule Forecasting Pipeline to run on the first day of every week\",\n",
    "#                             pipeline_id=training_pipeline_id, \n",
    "#                             experiment_name=experiment.name, \n",
    "#                             recurrence=recurrence)"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "roastala"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('vscpy39': conda)",
   "name": "python391jvsc74a57bd03f5a1eed9b4ace55e909398ac91c318c6556c0b2dba69b98588c80e77c44e826"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}